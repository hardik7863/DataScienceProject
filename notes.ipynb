{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Initial Step**\n",
    "\n",
    "setup the project structure using template.py\n",
    "\n",
    "then setup the logging in the src/datascience/__init__.py file \n",
    "you can even create the logging folder instead\n",
    "\n",
    "now check the logging in the main.py\n",
    "\n",
    "instead of using custom exception handling we have used python-box library \n",
    "\n",
    "adding common functionality in src/datascience/utils/common.py\n",
    "\n",
    "there are two types of model library pickle and joblib we will be using joblib here \n",
    "\n",
    "configbox handles exception better with yaml\n",
    "\n",
    "ensure annotation and config box is explain in research/research.ipynb\n",
    "\n",
    "updated the Readme file \n",
    "\n",
    "created 1_data_ingestion.ipynb in research folder and adding steps of data ingestion\n",
    "\n",
    "    root_dir: Path (create the director where dataset will be kept)\n",
    "    source_URL: str (path of the dataset file is kept in zip format)\n",
    "    local_data_file: Path (download the zip file at this location )\n",
    "    unzip_dir: Path (unzip the file at this location)\n",
    "\n",
    "normal class vs dataclass \n",
    "\n",
    "in normal class we use self keyword and in data class we do not use self keyword and data class is more focused on assigning values to the variable it do not have functions at all in data class.\n",
    "\n",
    "now we will define all the constants in constants/__intit__.py file\n",
    "\n",
    "after completing 1_data_ingestion.ipynb update the code it into modular format \n",
    "\n",
    "dataclass --> entity/config_entity.py \n",
    "\n",
    "dataconfigmanager --> config/configuration.py\n",
    "\n",
    "class dataingestion --> components/data_ingestion.py\n",
    "\n",
    "last block of code of 1_data_ingestion.ipynb   ---> data_ingestion_pipeline.py\n",
    "\n",
    "check the pipeline from main.py \n",
    "\n",
    "Data validation:- once we train our specific model. we have some fixed number of features i.e the input features and output features \n",
    "but whenever we get a new data to do the prediction those features are not same usually we want the data and data type of the new feature should be of certain type or form that our model perform will \n",
    "\n",
    "updated the path in config.yaml file for data vcalidation \n",
    "\n",
    "now create the research/2_data_validation.ipynb file\n",
    "\n",
    "then updateing schema.yaml file (content is get through EDA of dataset) \n",
    "\n",
    "and same worflow \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
